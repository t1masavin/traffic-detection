{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder, GTSRB\n",
    "from torchvision.transforms import transforms as T\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from PIL import Image\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join\n",
    "import kornia as K\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = GTSRB(root='./data/GTSRB/',download=True) # 43 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.2/28.06.2022 full____NO20220628-175643-000064F_1_1652.jpg',\n",
       " '/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.2/3____Screen (214)_375.jpg',\n",
       " '/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.2/28.06.2022 full____NO20220628-135134-000021F_8_253.jpg',\n",
       " '/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.2/27062022____NO20220627-161526-000096F_17_2182.jpg',\n",
       " '/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.2/27062022____NO20220627-130026-000031F_25_599.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs'\n",
    "jpg = glob.glob(PATH + '/*/*.jpg')\n",
    "jpg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(list_path):\n",
    "    return [\n",
    "        [\n",
    "            item,\n",
    "            item.split(\"/\")[-2]\n",
    "        ]\n",
    "        for item in list_path\n",
    "    ]\n",
    "\n",
    "\n",
    "def class2label(item):\n",
    "    if \"noise\" in item:\n",
    "        return \"noise\"\n",
    "    if len(set(item.split(\"/\"))) == len(item.split(\"/\")):\n",
    "        ret = item.split(\"/\")[1] if len(item.split(\"/\")) > 1 else item\n",
    "    else:\n",
    "        ret = item.split(\"/\")[-1]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def build_DataFrame(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.rename({0: \"dir\", 1: \"class\"}, axis=1)\n",
    "    # df.drop_duplicates(subset=[\"name\"], inplace=True)\n",
    "    # df[\"label\"] = df[\"class\"].apply(class2label)\n",
    "    # df[\"dir\"] = df.apply(lambda x: join(PATH, x[\"class\"], x[\"path\"], x[\"name\"]), axis=1)\n",
    "    # df = df[[\"dir\", \"label\"]]\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def build_label(data):\n",
    "    # uniq = data.iloc[:, 1:].columns\n",
    "    # len_uniq = len(uniq)\n",
    "    # range_uniq = range(len_uniq)\n",
    "    # d_ = dict(zip(range_uniq, uniq))\n",
    "    # weights = data.iloc[:, 1:].values.sum() / data.iloc[:, 1:].sum()\n",
    "    # return d_, list(weights)\n",
    "    uniq = data['class'].unique()\n",
    "    len_uniq = len(uniq)\n",
    "    range_uniq = range(len_uniq)\n",
    "    d_ = dict(zip(range_uniq, uniq))\n",
    "    d = dict(zip(uniq, range_uniq))\n",
    "    # weights = list(data.iloc[:, 1:].values.sum() / data.iloc[:, 1:].sum()\n",
    "    return d_, d\n",
    "\n",
    "def image_to_tensor(path):\n",
    "    tensor = Image.open(path)\n",
    "    return T.ToTensor()(tensor).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 dir class\n",
       "0  /root/workspace/work/Digital-Tashkent/Signs/tr...   1.2\n",
       "1  /root/workspace/work/Digital-Tashkent/Signs/tr...   1.2\n",
       "2  /root/workspace/work/Digital-Tashkent/Signs/tr...   1.2\n",
       "3  /root/workspace/work/Digital-Tashkent/Signs/tr...   1.2\n",
       "4  /root/workspace/work/Digital-Tashkent/Signs/tr...   1.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parse_data(jpg)\n",
    "df = build_DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dir</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>/root/workspace/work/Digital-Tashkent/Signs/tr...</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    dir class\n",
       "3520  /root/workspace/work/Digital-Tashkent/Signs/tr...  1.20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2label, label2num = build_label(df)\n",
    "sampledf = df.sample(weights = 1./df.groupby('class')['class'].transform('count'))\n",
    "sampledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/workspace/work/Digital-Tashkent/Signs/traffic-sign-recognition/classification/data/signs/1.20/27062022____NO20220627-134826-000047F_1_987.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(weights = 1./df.groupby('class')['class'].transform('count'))['dir'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         18\n",
       "1         18\n",
       "2         18\n",
       "3         18\n",
       "4         18\n",
       "        ... \n",
       "12377    308\n",
       "12378    308\n",
       "12379    308\n",
       "12380      2\n",
       "12381      2\n",
       "Name: class, Length: 12382, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('class')['class'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1 = pd.get_dummies(df, columns=['class'])\n",
    "# num2label, weights = build_label(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess(nn.Module):\n",
    "    \"\"\"Module to perform pre-process using Kornia on torch tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, keepdim=True) -> None:\n",
    "        super().__init__()\n",
    "        self.preproc = nn.Sequential(\n",
    "            K.augmentation.PadTo((50,50), keepdim=keepdim),\n",
    "            K.augmentation.RandomAffine(\n",
    "                degrees=20,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.7, 1.4),\n",
    "                p=0.8,\n",
    "                keepdim=keepdim\n",
    "            )\n",
    "        )\n",
    "        self.resize1= K.augmentation.LongestMaxSize(50)\n",
    "\n",
    "    @torch.no_grad()  # disable gradients for effiency\n",
    "    def forward(self, x_out: torch.Tensor) -> torch.Tensor:\n",
    "        x_out = self.resize1(x_out.to(torch.float))\n",
    "        x_out = self.preproc(x_out)\n",
    "\n",
    "        # return x_out.to(torch.float16)\n",
    "        return x_out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Torch DataSet for image loading and preprocessing.\n",
    "        :data: pandas.DataFrame\n",
    "        :root: str\n",
    "        :transforms: nn.Sequential\n",
    "        :label2num: dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        transforms: nn.Sequential,\n",
    "        label2num=None,\n",
    "        root: str = \"\",\n",
    "        \n",
    "    ):\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "        self.label2num = label2num\n",
    "        self.root = root\n",
    "\n",
    "    def parse_data(self, data):\n",
    "        path = data[\"dir\"].values[0]\n",
    "        img, label = image_to_tensor(path), data[\"class\"].values[0]\n",
    "        return img, label\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.sample(weights = 1./self.data.groupby('class')['class'].transform('count'))\n",
    "        img, label = self.parse_data(data)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        # print(label, int(self.label2num[str(label)]))\n",
    "        # torch.tensor(int(self.label2num[str(label)]), dtype=torch.float16)\n",
    "        return img.to(torch.float16), torch.tensor(self.label2num[label], dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset(df, PreProcess(), label2num)\n",
    "# T.ToPILImage()(ds[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0928, 0.0778, 0.0770,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0862, 0.0779, 0.0905,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0793, 0.0772, 0.1073,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1300, 0.1741, 0.2124,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1398, 0.1752, 0.2057,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1453, 0.1608, 0.2001,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0870, 0.0789, 0.0817,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0750, 0.0770, 0.0933,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0635, 0.0740, 0.1073,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.2181, 0.2507, 0.3000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2211, 0.2494, 0.2869,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2437, 0.2491, 0.2839,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1699, 0.1704, 0.1588,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1637, 0.1682, 0.1643,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1580, 0.1661, 0.1749,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1832, 0.2238, 0.2725,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1875, 0.2229, 0.2576,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2072, 0.2185, 0.2505,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        dtype=torch.float16),\n",
       " tensor(35))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(ds, 2000, num_workers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"../lightning_logs/cls/\", save_top_k=2, monitor=\"val_epoch_total_step\", mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtimasaviin\u001b[0m (\u001b[33mtsu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2d65cffd2949ce8157c874debc8d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668488499999513, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230215_145906-hu0fvfrn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tsu/cls_signs/runs/hu0fvfrn' target=\"_blank\">sparkling-valentine-3</a></strong> to <a href='https://wandb.ai/tsu/cls_signs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tsu/cls_signs' target=\"_blank\">https://wandb.ai/tsu/cls_signs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tsu/cls_signs/runs/hu0fvfrn' target=\"_blank\">https://wandb.ai/tsu/cls_signs/runs/hu0fvfrn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project='cls_signs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet(NUM_CLASSES):\n",
    "    model = M.resnet18(weights=M.ResNet18_Weights.DEFAULT)\n",
    "    in_feat = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features=in_feat, out_features=NUM_CLASSES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_resnet(len(num2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.cross_entropy = nn.BCEWithLogitsLoss(\n",
    "        #     pos_weight=self.get_pos_weight(weights), \n",
    "        #     reduction=\"none\"\n",
    "        # )\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    def get_pos_weight(self, weights):\n",
    "        pos_weight = torch.tensor(weights) if type(weights) == list else None\n",
    "        return pos_weight\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.1)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.3, patience=3), 'monitor': 'train_loss', }\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.cross_entropy(logits, targets)\n",
    "        #################################\n",
    "        # pt = torch.exp(-loss)\n",
    "        # loss = (2 * (1-pt)**1 * loss).sum()\n",
    "        self.log('train_loss', loss.detach(), on_step=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator='gpu', devices=1,\n",
    "                  max_epochs=1000,\n",
    "                  precision=16,\n",
    "                  log_every_n_steps=5,\n",
    "                  logger=wandb_logger,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | ResNet           | 11.2 M\n",
      "1 | sigmoid       | Sigmoid          | 0     \n",
      "2 | cross_entropy | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "22.467    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e449d71b6904da69d8dc1d853e6cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sign-rec/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = FasterRCNN()\n",
    "trainer.fit(model,\n",
    "            loader,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDataset' object has no attribute 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m label2num \u001b[39m=\u001b[39m {v:k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m ds\u001b[39m.\u001b[39;49mclass_to_idx\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDataset' object has no attribute 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "# label2num = {v:k for k, v in ds.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.4', 95)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=8790\n",
    "img = ds[i]\n",
    "num2label[int(img[1])], int(img[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5.16.2', 59)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2label[model(img[0].float().cuda().unsqueeze(0)).argmax().item()], model(img[0].float().cuda().unsqueeze(0)).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1.2',\n",
       " 1: '1.13',\n",
       " 2: '3.26',\n",
       " 3: '7.13',\n",
       " 4: '4.2.2',\n",
       " 5: '5.6',\n",
       " 6: '7.3.1',\n",
       " 7: '4.5',\n",
       " 8: '6.3',\n",
       " 9: '1.11.1',\n",
       " 10: '3.12',\n",
       " 11: '7.2.3',\n",
       " 12: '5.17.2',\n",
       " 13: '3.13',\n",
       " 14: '4.1.1',\n",
       " 15: '1.31.2',\n",
       " 16: '5.19.1',\n",
       " 17: '3.24',\n",
       " 18: '4.11.1',\n",
       " 19: '1.4.3',\n",
       " 20: '5.41',\n",
       " 21: '1.11.2',\n",
       " 22: '5.15',\n",
       " 23: '3.18.1',\n",
       " 24: '5.1.11',\n",
       " 25: '5.8.3',\n",
       " 26: '5.19.3',\n",
       " 27: '3.19',\n",
       " 28: '5.5',\n",
       " 29: '7.4.1',\n",
       " 30: '4.1.4',\n",
       " 31: '1.4.1',\n",
       " 32: '1.3.1',\n",
       " 33: '5.38',\n",
       " 34: '2.5',\n",
       " 35: '3.5',\n",
       " 36: '7.21',\n",
       " 37: '5.42',\n",
       " 38: '3.10',\n",
       " 39: '1.1',\n",
       " 40: '4.6.3',\n",
       " 41: '6.4',\n",
       " 42: '7.2.4',\n",
       " 43: '5.12',\n",
       " 44: '5.43',\n",
       " 45: '7.22.3',\n",
       " 46: '2.4.',\n",
       " 47: '3.28',\n",
       " 48: '4.6.1',\n",
       " 49: '5.33',\n",
       " 50: '1.4.2',\n",
       " 51: '1.30',\n",
       " 52: '1.4.4',\n",
       " 53: '4.3',\n",
       " 54: '1.31',\n",
       " 55: '5.8.5',\n",
       " 56: '4.1.2',\n",
       " 57: '1.20',\n",
       " 58: '1.12.2',\n",
       " 59: '5.16.2',\n",
       " 60: '4.6',\n",
       " 61: '2.3.2',\n",
       " 62: '1.7',\n",
       " 63: '1.21',\n",
       " 64: '7.22.2',\n",
       " 65: '3.20',\n",
       " 66: '1.4.6',\n",
       " 67: '1.31.1',\n",
       " 68: '5.29.1',\n",
       " 69: '7.6.1',\n",
       " 70: '5.7.1',\n",
       " 71: '1.3.2',\n",
       " 72: '5.8.1',\n",
       " 73: '3.25',\n",
       " 74: '3.18.2',\n",
       " 75: '4.2.1',\n",
       " 76: '1.8',\n",
       " 77: '1.29',\n",
       " 78: '4.6.5',\n",
       " 79: '4.2.3',\n",
       " 80: '4.1.3',\n",
       " 81: '5.46',\n",
       " 82: '1.18.3',\n",
       " 83: '1.18.2',\n",
       " 84: '5.16.1',\n",
       " 85: '2.2',\n",
       " 86: '3.27',\n",
       " 87: '7.2.1',\n",
       " 88: '3.1',\n",
       " 89: '7.3.2',\n",
       " 90: '1.4.5',\n",
       " 91: '5.8.6',\n",
       " 92: '4.1.5',\n",
       " 93: '3.2',\n",
       " 94: '5.8.2',\n",
       " 95: '3.4',\n",
       " 96: '2.3.3',\n",
       " 97: '5.9',\n",
       " 98: '1.23',\n",
       " 99: '2.1',\n",
       " 100: '7.3.3',\n",
       " 101: '7.15',\n",
       " 102: '2.6',\n",
       " 103: '7.22.1',\n",
       " 104: '5.17.1',\n",
       " 105: '5.7.2',\n",
       " 106: '5.19.2',\n",
       " 107: '4.14',\n",
       " 108: '6.12',\n",
       " 109: '5.11.1',\n",
       " 110: '7.4.3'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f842aa1788f26e6ab38c014779ece966b3b73b041a52ee3daa1e6cc3a3c21458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
